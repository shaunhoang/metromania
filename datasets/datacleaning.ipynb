{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "37e860ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "######## Data intake\n",
    "cities = pd.read_csv(\"datasets/cities.csv\")\n",
    "stations = pd.read_csv(\"datasets/stations.csv\")\n",
    "tracks = pd.read_csv(\"datasets/tracks.csv\")\n",
    "lines = pd.read_csv(\"datasets/lines.csv\")\n",
    "track_lines = pd.read_csv(\"datasets/track_lines.csv\")\n",
    "station_lines = pd.read_csv(\"datasets/station_lines.csv\")\n",
    "systems = pd.read_csv(\"datasets/systems.csv\")\n",
    "modes = pd.read_csv(\"datasets/modes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c2b002fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select and rename columns needed for merging in one go\n",
    "cities_info = cities[['id', 'country', 'name']].rename(columns={'id': 'city_id', 'name': 'city'})\n",
    "lines_info = lines[['id', 'name', 'color', 'system_id','transport_mode_id']].rename(columns={'id': 'line_id', 'name': 'line_name', 'color': 'line_color'})\n",
    "systems_info = systems[['id', 'name']].rename(columns={'id': 'system_id', 'name': 'system_name'})\n",
    "track_lines_info = track_lines[['section_id', 'line_id','fromyear','toyear']]\n",
    "station_lines_info = station_lines[['station_id', 'line_id','fromyear','toyear']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0bafcd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get mode for each line from modes and then merge \n",
    "modes_info = modes[['id', 'name']].rename(columns={'id': 'transport_mode_id', 'name': 'transport_mode_name'})\n",
    "modes_info['transport_mode_name'] = modes_info['transport_mode_name'].str.replace('_', ' ')\n",
    "modes_info['transport_mode_name'] = modes_info['transport_mode_name'].str.title()\n",
    "\n",
    "lines_info = lines_info.merge(modes_info, on='transport_mode_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b5980510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Process STATIONS ---\n",
    "stations = stations.rename(columns={'id': 'station_id', 'name': 'station_name'})\n",
    "\n",
    "# Merge\n",
    "stations = (stations\n",
    "            .merge(cities_info, on='city_id', how='left')\n",
    "            .merge(station_lines_info, on='station_id', how='left')\n",
    "            .merge(lines_info, on='line_id', how='left')\n",
    "            .merge(systems_info, on='system_id', how='left')\n",
    "           )\n",
    "\n",
    "# Extract coordinates\n",
    "stations['longitude'] = stations['geometry'].str.extract(r'POINT\\(([^ ]+) [^)]+\\)')[0].astype(float)\n",
    "stations['latitude'] = stations['geometry'].str.extract(r'POINT\\([^ ]+ ([^)]+)\\)')[0].astype(float)\n",
    "\n",
    "station_cols = ['station_id', 'station_name', 'geometry', 'longitude', 'latitude',\n",
    "                'opening', 'closure', 'city_id', 'city', 'country',\n",
    "                'line_id', 'line_name', 'line_color', 'system_id', 'system_name',\n",
    "                'transport_mode_id', 'transport_mode_name','fromyear','toyear']\n",
    "stations = stations[station_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ec9c5efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Process TRACKS ---\n",
    "tracks = tracks.rename(columns={'id': 'section_id'})\n",
    "\n",
    "# Merge\n",
    "tracks = (tracks\n",
    "          .merge(cities_info, on='city_id', how='left')\n",
    "          .merge(track_lines_info, on='section_id', how='left')\n",
    "          .merge(lines_info, on='line_id', how='left')\n",
    "          .merge(systems_info, on='system_id', how='left')\n",
    "         )\n",
    "\n",
    "track_cols = ['section_id', 'geometry', 'opening', 'closure', 'length',\n",
    "              'line_id', 'line_name', 'line_color', 'system_id', 'system_name',\n",
    "              'city_id', 'city', 'country', 'transport_mode_id', 'transport_mode_name','fromyear','toyear']\n",
    "tracks = tracks[track_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "eec8211a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Handle name missing data ---\n",
    "\n",
    "## Fill in NA \n",
    "stations['station_name'] = stations['station_name'].fillna('')\n",
    "stations['line_name'] = stations['line_name'].fillna('')\n",
    "stations['line_color'] = stations['line_color'].fillna('#000000')\n",
    "tracks['line_name'] = tracks['line_name'].fillna('')\n",
    "tracks['line_color'] = tracks['line_color'].fillna('#000000')\n",
    "stations['line_id'] = stations['line_id'].fillna(0)\n",
    "tracks['line_id'] = tracks['line_id'].fillna(0)\n",
    "\n",
    "# --- Handle opening and closure date missing data ---\n",
    "\n",
    "# Handle Closure Dates, Fill NaN with 999999, \"still open\".\n",
    "stations['closure'] = stations['closure'].fillna(999999)\n",
    "tracks['closure'] = tracks['closure'].fillna(999999)\n",
    "\n",
    "# Handle Opening Dates, impute missing data based on minimum known opening year for each city\n",
    "stations['opening'] = pd.to_numeric(stations['opening'], errors='coerce')\n",
    "tracks['opening'] = pd.to_numeric(tracks['opening'], errors='coerce')\n",
    "stations.loc[stations['opening'] > 2100, 'opening'] = pd.NA\n",
    "tracks.loc[tracks['opening'] > 2100, 'opening'] = pd.NA\n",
    "stations.loc[stations['opening'] == 0, 'opening'] = pd.NA\n",
    "tracks.loc[tracks['opening'] == 0, 'opening'] = pd.NA\n",
    "stations['opening'] = stations['opening'].astype('Int64')\n",
    "tracks['opening'] = tracks['opening'].astype('Int64')\n",
    "\n",
    "# Impute\n",
    "stations['min_city_opening'] = stations.groupby('city')['opening'].transform('min')\n",
    "tracks['min_city_opening'] = tracks.groupby('city')['opening'].transform('min')\n",
    "stations['opening'] = stations['opening'].fillna(stations['min_city_opening'])\n",
    "tracks['opening'] = tracks['opening'].fillna(tracks['min_city_opening'])\n",
    "# Handle cities where all dates were missing (min_city_opening is NaN)\n",
    "stations['opening'] = stations['opening'].fillna(1900).astype(int) \n",
    "tracks['opening'] = tracks['opening'].fillna(1900).astype(int) \n",
    "stations = stations.drop(columns=['min_city_opening']) # Drop the temporary column\n",
    "tracks = tracks.drop(columns=['min_city_opening']) # Drop the temporary column\n",
    "\n",
    "# Flag rows still missing opening date after imputation\n",
    "stations['was_missing_opening'] = stations['opening'].isna() \n",
    "tracks['was_missing_opening'] = tracks['opening'].isna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7fe721d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ... (wonkiness calculation) ...\n",
    "# Calculate proportion of missing/imputed opening dates per city\n",
    "pivot_st = pd.pivot_table(stations, values='station_id', index='city_id', columns='was_missing_opening', aggfunc='count', fill_value=0)\n",
    "pivot_tr = pd.pivot_table(tracks, values='section_id', index='city_id', columns='was_missing_opening', aggfunc='count', fill_value=0)\n",
    "\n",
    "pivot_st = pivot_st.rename(columns={True: 'missing', False: 'valid'})\n",
    "pivot_tr = pivot_tr.rename(columns={True: 'missing', False: 'valid'})\n",
    "if 'missing' not in pivot_st.columns: pivot_st['missing'] = 0\n",
    "if 'valid' not in pivot_st.columns: pivot_st['valid'] = 0\n",
    "if 'missing' not in pivot_tr.columns: pivot_tr['missing'] = 0\n",
    "if 'valid' not in pivot_tr.columns: pivot_tr['valid'] = 0\n",
    "\n",
    "# Calculate the \"wonkiness\" score 9missing dates) then merge for filtering\n",
    "pivot_st['wonkiness_st'] = pivot_st['missing'] / (pivot_st['valid'] + pivot_st['missing'])\n",
    "pivot_tr['wonkiness_tr'] = pivot_tr['missing'] / (pivot_tr['valid'] + pivot_tr['missing'])\n",
    "wonk_table = pd.merge(pivot_st[['valid', 'missing', 'wonkiness_st']],\n",
    "                      pivot_tr[['valid', 'missing', 'wonkiness_tr']],\n",
    "                      on='city_id', suffixes=('_st', '_tr'))\n",
    "\n",
    "wonk_table = wonk_table.merge(cities_info[['city_id','city', 'country']], on='city_id', how='left')\n",
    "wonk_table['total_stations'] = wonk_table['valid_st'] + wonk_table['missing_st']\n",
    "wonk_table['total_tracks'] = wonk_table['valid_tr'] + wonk_table['missing_tr']\n",
    "\n",
    "# wonk_table['avg_wonkiness'] = (wonk_table['wonkiness_st'] + wonk_table['wonkiness_tr']) / 2 \n",
    "wonk_table['avg_wonkiness'] = wonk_table['wonkiness_tr'] # wonk based on tracks only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa081227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Extract LineString coordinates from WKT strings ---\n",
    "import re\n",
    "def parse_linestring_coords(wkt_string, swap_order=False, precision=6):\n",
    "    if not isinstance(wkt_string, str):\n",
    "        return None \n",
    "\n",
    "    # Regular expression to find all coordinate pairs (lon lat)\n",
    "    coord_pattern = re.compile(r\"(-?\\d+\\.?\\d*)\\s+(-?\\d+\\.?\\d*)\")\n",
    "    matches = coord_pattern.findall(wkt_string)\n",
    "    if not matches:\n",
    "        return None\n",
    "\n",
    "    coord_list = []\n",
    "    for lon_str, lat_str in matches:\n",
    "        try:\n",
    "            lon = round(float(lon_str), precision)\n",
    "            lat = round(float(lat_str), precision)\n",
    "            if swap_order:\n",
    "                coord_list.append([lat, lon]) # Append as [lat, lon]\n",
    "            else:\n",
    "                coord_list.append([lon, lat]) # Append as [lon, lat]\n",
    "        except ValueError:\n",
    "            print(f\"WARN: Could not parse coordinate pair: ('{lon_str}', '{lat_str}') in '{wkt_string}'\")\n",
    "            continue \n",
    "\n",
    "    return coord_list if coord_list else None\n",
    "\n",
    "tracks['linestring_lonlat'] = tracks['geometry'].apply(\n",
    "    lambda x: parse_linestring_coords(x, swap_order=False)\n",
    ") # to be used for export\n",
    "tracks['linestring_latlon'] = tracks['geometry'].apply(\n",
    "    lambda x: parse_linestring_coords(x, swap_order=True)\n",
    ") # to be used for map plotting dl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "330e172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "stations.to_csv(\n",
    "  \"stations_cleaned.csv\", \n",
    "  index=False, \n",
    "  encoding='utf-8', \n",
    "  )\n",
    "tracks.to_csv(\n",
    "  \"tracks_cleaned.csv\", \n",
    "  index=False, \n",
    "  encoding='utf-8', \n",
    "  )\n",
    "wonk_table.to_csv(\n",
    "  \"city_wonkiness.csv\", \n",
    "  index=True,\n",
    "  encoding='utf-8', \n",
    "  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
